{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0e205a",
   "metadata": {},
   "source": [
    "# AutoGen\n",
    "\n",
    "DOC [LiteLLM with Ollama](https://microsoft.github.io/autogen/0.2/docs/topics/non-openai-models/local-litellm-ollama)\n",
    "\n",
    "!litellm --model ollama/llama3:latest\n",
    "\n",
    "This will run the proxy server and it will be available at ‘http://0.0.0.0:4000/’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4696f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llama3:latest']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "reply = ollama.list() # list all models\n",
    "[model.model for model in reply.models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32598d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "How can I help you today?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Tell me a joke\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile! Do you want another one?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (ae4ffb6e-519d-4224-8b41-f1fe53714936): User requested to end the conversation\u001b[0m\n",
      "agent\n"
     ]
    }
   ],
   "source": [
    "# Single-agent example using Ollama LiteLLM\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "local_llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"NotRequired\",  \n",
    "            \"api_key\": \"NotRequired\", \n",
    "            \"base_url\": \"http://localhost:4000\",  \n",
    "            \"price\": [0, 0],  \n",
    "        }\n",
    "    ],\n",
    "    \"cache_seed\": None, \n",
    "}\n",
    "assistant = ConversableAgent(\"agent\", llm_config=local_llm_config)\n",
    "user_proxy = UserProxyAgent(\"user\", code_execution_config=False)\n",
    "res = assistant.initiate_chat(user_proxy, message=\"How can I help you today?\")\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e850dc",
   "metadata": {},
   "source": [
    "## Tool Use with Gemini\n",
    "\n",
    "DOC [Tool Use](https://microsoft.github.io/autogen/0.2/docs/tutorial/tool-use)\n",
    "\n",
    "[Avaiable Models](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tools\n",
    "import os\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "Operator = Literal[\"+\", \"-\", \"*\", \"/\"]\n",
    "\n",
    "def calculator(a: int, b: int, operator: Annotated[Operator, \"operator\"]) -> int:\n",
    "    if operator == \"+\":\n",
    "        return a + b\n",
    "    elif operator == \"-\":\n",
    "        return a - b\n",
    "    elif operator == \"*\":\n",
    "        return a * b\n",
    "    elif operator == \"/\":\n",
    "        return int(a / b)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operator\")\n",
    "    \n",
    "with open('gemini_key.txt', 'r') as f:\n",
    "    gemini_key = f.read().strip()\n",
    "\n",
    "gemini_llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"gemini-2.0-flash\", \n",
    "            \"api_key\": gemini_key,  \n",
    "            \"api_type\": \"google\",\n",
    "        }\n",
    "    ],\n",
    "    \"cache_seed\": None,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11272250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'description': 'A simple calculator',\n",
       "   'name': 'calculator',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'a': {'type': 'integer', 'description': 'a'},\n",
       "     'b': {'type': 'integer', 'description': 'b'},\n",
       "     'operator': {'enum': ['+', '-', '*', '/'],\n",
       "      'type': 'string',\n",
       "      'description': 'operator'}},\n",
       "    'required': ['a', 'b', 'operator']}}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Registering Tools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "assistant = ConversableAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"You are a helpful AI assistant. \"\n",
    "    \"You can help with simple calculations. \"\n",
    "    \"After answering, reply with 'TERMINATE' to end the conversation.\",\n",
    "    llm_config=gemini_llm_config,\n",
    ")\n",
    "\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "assistant.register_for_llm(name=\"calculator\", description=\"A simple calculator\")(calculator)\n",
    "user_proxy.register_for_execution(name=\"calculator\")(calculator)\n",
    "\n",
    "from autogen import register_function\n",
    "register_function(\n",
    "    calculator,\n",
    "    caller=assistant,  \n",
    "    executor=user_proxy,  \n",
    "    name=\"calculator\",  \n",
    "    description=\"A simple calculator\",  \n",
    ")\n",
    "assistant.llm_config[\"tools\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62cc8b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "4 + (5 * 3)?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (3757): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"b\": 3, \"a\": 5, \"operator\": \"*\"}\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\n",
      "Call ID: 3757\n",
      "Input arguments: {'b': 3, 'a': 5, 'operator': '*'}\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTED FUNCTION calculator...\n",
      "Call ID: 3757\n",
      "Input arguments: {'b': 3, 'a': 5, 'operator': '*'}\n",
      "Output:\n",
      "15\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (3757) *****\u001b[0m\n",
      "15\n",
      "\u001b[32m*********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (1098): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\": 4, \"b\": 15, \"operator\": \"+\"}\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\n",
      "Call ID: 1098\n",
      "Input arguments: {'a': 4, 'b': 15, 'operator': '+'}\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTED FUNCTION calculator...\n",
      "Call ID: 1098\n",
      "Input arguments: {'a': 4, 'b': 15, 'operator': '+'}\n",
      "Output:\n",
      "19\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (1098) *****\u001b[0m\n",
      "19\n",
      "\u001b[32m*********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "19\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (35df58ad-bb7b-4909-b819-46db0b94cee3): Termination message condition on agent 'User' met\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Using Tools\n",
    "chat_result = user_proxy.initiate_chat(assistant, message=\"4 + (5 * 3)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c755a42",
   "metadata": {},
   "source": [
    "Structured output - Chain-of-Thought\n",
    "\n",
    "DOC [Agents](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e743d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "I'm not feeling well.\n",
      "---------- StructuredMessage[AgentResponse] (assistant) ----------\n",
      "{\"thoughts\":\"The user reported feeling unwell, suggesting a negative emotional state.\",\"response\":\"sad\"}\n",
      "Thought:  The user reported feeling unwell, suggesting a negative emotional state.\n",
      "Response:  sad\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import StructuredMessage\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    thoughts: str\n",
    "    response: Literal[\"happy\", \"sad\", \"neutral\"]\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=gemini_key, \n",
    ")\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    \"assistant\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Categorize the input as happy, sad, or neutral following the JSON format.\",\n",
    "    output_content_type=AgentResponse,\n",
    ")\n",
    "\n",
    "result = await Console(agent.run_stream(task=\"I'm not feeling well.\"))\n",
    "\n",
    "assert isinstance(result.messages[-1], StructuredMessage)\n",
    "assert isinstance(result.messages[-1].content, AgentResponse)\n",
    "print(\"Thought: \", result.messages[-1].content.thoughts)\n",
    "print(\"Response: \", result.messages[-1].content.response)\n",
    "await model_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd2931a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"Find information on the web\"\"\"\n",
    "    return \"AutoGen is a programming framework for building multi-agent applications.\"\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=gemini_key, \n",
    ")\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    tools=[web_search],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 18, 15, 15, 10, 788622, tzinfo=datetime.timezone.utc), content='Find information on AutoGen', type='TextMessage'), ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=52, completion_tokens=6), metadata={}, created_at=datetime.datetime(2025, 6, 18, 15, 15, 11, 302451, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='', arguments='{\"query\":\"AutoGen\"}', name='web_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 18, 15, 15, 11, 302451, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', name='web_search', call_id='', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 18, 15, 15, 11, 302451, tzinfo=datetime.timezone.utc), content='AutoGen is a programming framework for building multi-agent applications.', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='', arguments='{\"query\":\"AutoGen\"}', name='web_search')], results=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', name='web_search', call_id='', is_error=False)])]\n",
      "AutoGen is a programming framework for building multi-agent applications.\n"
     ]
    }
   ],
   "source": [
    "result = await agent.run(task=\"Find information on AutoGen\")\n",
    "print(result.messages)\n",
    "print(result.messages[-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
